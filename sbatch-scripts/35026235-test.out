Found 2 GPU cards for training
Choose random seed 8627
loading dictionary from ./data/glove/dictionary.pkl
loading features from h5 file ./data/Bottom-up-features-adaptive/val.hdf5
Setting semantic adj matrix to None...
Setting spatial adj matrix to None...
loading features from h5 file ./data/Bottom-up-features-adaptive/train.hdf5
Setting semantic adj matrix to None...
Setting spatial adj matrix to None...
Building ReGAT model with implicit relation and butd fusion method
In ImplicitRelationEncoder, num of graph propogate steps: 1, residual_connection: True
embedding dim is 300
tf-idf stochastic matrix (19901 x 28333) is generated.
DataParallel(
  (module): ReGAT(
    (w_emb): WordEmbedding(
      (emb): Embedding(19902, 300, padding_idx=19901)
      (emb_): Embedding(19902, 300, padding_idx=19901)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (q_emb): QuestionEmbedding(
      (rnn): GRU(600, 1024, batch_first=True)
    )
    (q_att): QuestionSelfAttention(
      (drop): Dropout(p=0.2, inplace=False)
      (W1_self_att_q): FCNet(
        (main): Sequential(
          (0): Dropout(p=0.2, inplace=False)
          (1): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (W2_self_att_q): FCNet(
        (main): Sequential(
          (0): Linear(in_features=1024, out_features=1, bias=True)
        )
      )
    )
    (v_relation): ImplicitRelationEncoder(
      (v_transform): FCNet(
        (main): Sequential(
          (0): Linear(in_features=2048, out_features=1024, bias=True)
          (1): ReLU()
        )
      )
      (implicit_relation): GAttNet(
        (dropout): Dropout(p=0.2, inplace=False)
        (self_weights): FCNet(
          (main): Sequential(
            (0): Dropout(p=0.2, inplace=False)
            (1): Linear(in_features=2048, out_features=1024, bias=True)
          )
        )
        (bias): FCNet(
          (main): Sequential(
            (0): Linear(in_features=1, out_features=1, bias=False)
          )
        )
        (neighbor_net): ModuleList(
          (0-1): 2 x GraphSelfAttentionLayer(
            (pair_pos_fc1): FCNet(
              (main): Sequential(
                (0): Dropout(p=0.2, inplace=False)
                (1): Linear(in_features=64, out_features=16, bias=True)
              )
            )
            (query): FCNet(
              (main): Sequential(
                (0): Dropout(p=0.2, inplace=False)
                (1): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (key): FCNet(
              (main): Sequential(
                (0): Dropout(p=0.2, inplace=False)
                (1): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (linear_out_): Conv2d(16384, 1024, kernel_size=(1, 1), stride=(1, 1), groups=16)
          )
        )
      )
    )
    (joint_embedding): BUTD(
      (v_proj): FCNet(
        (main): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): ReLU()
        )
      )
      (q_proj): FCNet(
        (main): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): ReLU()
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (linear): FCNet(
        (main): Sequential(
          (0): Linear(in_features=1024, out_features=1, bias=True)
          (1): ReLU()
        )
      )
      (q_net): FCNet(
        (main): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): ReLU()
        )
      )
      (v_net): FCNet(
        (main): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): ReLU()
        )
      )
    )
    (classifier): SimpleClassifier(
      (main): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): ReLU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=2048, out_features=3129, bias=True)
      )
    )
  )
)
nParams=	41192896
optim: adamax lr=0.0010, decay_step=2, decay_rate=0.25,grad_clip=0.25
LR decay epochs: 15,17,19
gradual warmup lr: 0.0005
epoch 0, time: 1407.04
	train_loss: 9.48, norm: 9.7082, score: 32.40
	eval score: 37.71 (92.66)
saving current model weights to folder
gradual warmup lr: 0.0010
epoch 1, time: 1253.80
	train_loss: 4.57, norm: 4.1716, score: 42.46
	eval score: 49.07 (92.66)
saving current model weights to folder
gradual warmup lr: 0.0015
epoch 2, time: 1263.38
	train_loss: 3.90, norm: 3.1494, score: 51.76
	eval score: 55.58 (92.66)
saving current model weights to folder
gradual warmup lr: 0.0020
epoch 3, time: 1256.59
	train_loss: 3.59, norm: 2.4207, score: 56.79
	eval score: 58.13 (92.66)
saving current model weights to folder
lr: 0.0020
epoch 4, time: 1261.74
	train_loss: 3.36, norm: 1.9655, score: 60.34
	eval score: 59.79 (92.66)
saving current model weights to folder
lr: 0.0020
epoch 5, time: 1255.91
	train_loss: 3.21, norm: 1.7878, score: 62.81
	eval score: 60.97 (92.66)
saving current model weights to folder
lr: 0.0020
epoch 6, time: 1257.61
	train_loss: 3.09, norm: 1.5612, score: 64.82
	eval score: 61.70 (92.66)
saving current model weights to folder
lr: 0.0020
epoch 7, time: 1257.08
	train_loss: 2.98, norm: 1.5761, score: 66.61
	eval score: 62.42 (92.66)
saving current model weights to folder
lr: 0.0020
epoch 8, time: 1257.27
	train_loss: 2.89, norm: 1.3988, score: 68.16
	eval score: 62.73 (92.66)
saving current model weights to folder
lr: 0.0020
epoch 9, time: 1254.65
	train_loss: 2.81, norm: 1.3690, score: 69.56
	eval score: 62.91 (92.66)
saving current model weights to folder
lr: 0.0020
epoch 10, time: 1254.73
	train_loss: 2.74, norm: 1.3451, score: 70.86
	eval score: 63.21 (92.66)
saving current model weights to folder
lr: 0.0020
epoch 11, time: 1256.43
	train_loss: 2.67, norm: 1.3745, score: 72.13
	eval score: 63.30 (92.66)
saving current model weights to folder
lr: 0.0020
epoch 12, time: 1253.49
	train_loss: 2.61, norm: 1.3225, score: 73.16
	eval score: 63.51 (92.66)
saving current model weights to folder
lr: 0.0020
epoch 13, time: 1258.92
	train_loss: 2.55, norm: 1.3324, score: 74.21
	eval score: 63.62 (92.66)
saving current model weights to folder
lr: 0.0020
epoch 14, time: 1255.85
	train_loss: 2.50, norm: 1.3201, score: 75.12
	eval score: 63.61 (92.66)
saving current model weights to folder
decreased lr: 0.0005
epoch 15, time: 1258.81
	train_loss: 2.31, norm: 1.1774, score: 78.00
	eval score: 64.14 (92.66)
saving current model weights to folder
lr: 0.0005
epoch 16, time: 1257.55
	train_loss: 2.24, norm: 1.2039, score: 79.02
	eval score: 64.20 (92.66)
saving current model weights to folder
decreased lr: 0.0001
epoch 17, time: 1260.56
	train_loss: 2.16, norm: 1.1642, score: 80.07
	eval score: 64.23 (92.66)
saving current model weights to folder
lr: 0.0001
epoch 18, time: 1253.68
	train_loss: 2.15, norm: 1.1729, score: 80.33
	eval score: 64.20 (92.66)
saving current model weights to folder
decreased lr: 0.0000
epoch 19, time: 1265.46
	train_loss: 2.12, norm: 1.1357, score: 80.63
	eval score: 64.18 (92.66)
saving current model weights to folder
